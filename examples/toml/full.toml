#                                    __   __  __
#                                    \ \ / / / /
#                                     \ V / / /
#                                      \_/  \/
#
#                                    V E C T O R
#                             Configuration Definition
#
# ------------------------------------------------------------------------------
# Website: https://vector.dev
# Docs: https://vector.dev/docs/
# Community: https://vector.dev/community/
# ------------------------------------------------------------------------------
# More info on Vector's configuration can be found at:
# /docs/setup/configuration/

# ------------------------------------------------------------------------------
# Global
# ------------------------------------------------------------------------------
# Global options are relevant to Vector as a whole and apply to global behavior.

data_dir = "/var/lib/vector"



# ------------------------------------------------------------------------------
# Sources
# ------------------------------------------------------------------------------
# Sources specify data sources and are responsible for ingesting data into
# Vector.

# ------------------------------------------------------------------------------
# Ingest data by tailing one or more files
[sources.apache_logs]
  ignore_older = 86400
  include = ["/var/log/apache2/*.log"]
  type = "file"



# ------------------------------------------------------------------------------
# Transforms
# ------------------------------------------------------------------------------
# Transforms parse, structure, and enrich events.

# ------------------------------------------------------------------------------
# This is only for multiline string purpose... This component do nothing
[transforms.apache_lua]
  inputs = [ ]
  type = "lua"
  version = "2"
  [transforms.apache_lua.hooks]
    process = """
    function (event, emit)
      -- event.log.field = \"value\" -- set value of a field
      -- event.log.another_field = nil -- remove field
      -- event.log.first, event.log.second = nil, event.log.first -- rename field
    
      -- Very important! Emit the processed event.
      emit(event)
    end
    
    """

# ------------------------------------------------------------------------------
[transforms.apache_parser]
  drop_on_error = false
  inputs = ["apache_logs"]
  source = """
  . |= parse_regex!(.message, r'^(?P<host>[\\w.]+) - (?P<user>[\\w]+) (?P<bytes_in>[\\d]+) [(?P<timestamp>.*)] \"(?P<method>[\\w]+) (?P<path>.*)\" (?P<status>[\\d]+) (?P<bytes_out>[\\d]+)$'),
  
  """
  type = "remap"

# ------------------------------------------------------------------------------
[transforms.apache_sampler]
  inputs = ["apache_parser"]
  rate = 50
  type = "sample"

# ------------------------------------------------------------------------------
# Extract somes metrics from the parsed logs
[transforms.log_to_metric]
  inputs = ["apache_parser"]
  type = "log_to_metric"
  [[transforms.log_to_metric.metrics]]
    field = "message"
    type = "counter"
  [[transforms.log_to_metric.metrics]]
    field = "bytes_out"
    increment_by_value = true
    name = "bytes_out_total"
    type = "counter"
  [[transforms.log_to_metric.metrics]]
    field = "bytes_out"
    type = "gauge"
  [[transforms.log_to_metric.metrics]]
    field = "user"
    type = "set"
  [[transforms.log_to_metric.metrics]]
    field = "bytes_out"
    name = "bytes_out_histogram"
    type = "histogram"



# ------------------------------------------------------------------------------
# Sinks
# ------------------------------------------------------------------------------
# Sinks batch or stream data out of Vector.

# ------------------------------------------------------------------------------
# Send structured data to a short-term storage
[sinks.es_cluster]
  host = "http://79.12.221.222:9200"
  index = "vector-%Y-%m-%d"
  inputs = ["apache_sampler"]
  type = "elasticsearch"

# ------------------------------------------------------------------------------
# Send metrics to Prometheus
[sinks.prometheus]
  buckets = [0, 10, 100, 1000, 10000, 100001]
  default_namespace = "vector"
  inputs = ["log_to_metric"]
  type = "prometheus_exporter"

# ------------------------------------------------------------------------------
# Send structured data to a cost-effective long-term storage.
# 
# AWS S3 is a good choice for this purpose.
[sinks.s3_archives]
  bucket = "my-log-archives"
  compression = "gzip"
  inputs = ["apache_parser"]
  key_prefix = "date=%Y-%m-%d"
  region = "us-east-1"
  type = "aws_s3"
  [sinks.s3_archives.batch]
    max_size = 10000000
  [sinks.s3_archives.encoding]
    codec = "json"


